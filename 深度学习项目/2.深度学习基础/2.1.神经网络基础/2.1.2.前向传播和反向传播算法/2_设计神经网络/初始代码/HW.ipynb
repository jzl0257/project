{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"HW.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yeIXFfAumNWV"},"source":["# Neural Network In Pytorch\n","This assignment aims to get familiarized with building the whole pipeline of deep learning in Pytorch to perform classification and test it out on the CIFAR-10 dataset. All the code will be implemented in this notebook."]},{"cell_type":"code","metadata":{"id":"_BD5dqNGm1b6"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3HggVCdm8iV"},"source":["%cd gdrive/MyDrive/Colab\\ Notebooks/HW3_Full"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"soWnE8qOuQwg"},"source":["First, let's install modules not already installed by Google Colab."]},{"cell_type":"code","metadata":{"id":"75XndHcNuL0r"},"source":["! pip install torch_utils"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mzuptv--6uXv"},"source":["## Task 1: Design the Neural Network and Data Preparation\n"]},{"cell_type":"markdown","metadata":{"id":"kTcfx8MFFxRz"},"source":["In the beginning, please import all the packages you need. We provide some packages here, which might be helpful when you build your code."]},{"cell_type":"code","metadata":{"id":"v4wg5xqcxNEi"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# import modules\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import cuda\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, sampler\n","import torch.nn.functional as F\n","from torch_utils import AverageMeter\n","import math\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","from numpy import inf\n","\n","\n","from sklearn.model_selection import train_test_split\n","import os\n","from glob import glob\n","from torchvision import transforms\n","from torchvision import datasets\n","from torchvision import models\n","from torch import optim, cuda, Tensor\n","import tqdm\n","\n","# Data science tools\n","import numpy as np\n","\n","import os\n","\n","# Image manipulations\n","from PIL import Image\n","from timeit import default_timer as timer\n","\n","# Visualizations\n","import matplotlib.pyplot as plt\n","#plt.rcParams['font.size'] = 14\n","\n","import warnings\n","warnings.filterwarnings('ignore', category=FutureWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPX2j9gCHFOW"},"source":["###Network Design\n","Then, we need to define a class for your network. The network should include two linear layer, one Relu layer, and one dropout layer."]},{"cell_type":"code","metadata":{"id":"xDl90_EICBFp"},"source":["# define model\n","class bmodel(nn.Module):\n","    def __init__(self, input_d, hidden_d, output_d):\n","        super().__init__()\n","        # Define all the layers that you need in your network\n","        # You can use nn.Linear() to define the linear layer\n","        # You can use nn.Dropout() to define the dropout layer\n","        # You can use F.relu() to define your ReLu layer\n","        ######################\n","        ### YOUR CODE HERE ###\n","        ######################\n","        \n","        #####################\n","        ### YOUR CODE END ###\n","        #####################\n","\n","    def forward(self, x):\n","        # Design your network structure here\n","        ######################\n","        ### YOUR CODE HERE ###\n","        ######################\n","        \n","        #####################\n","        ### YOUR CODE END ###\n","        #####################\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZiqsgPSH24o"},"source":["Now, we can design our toy model to test your network."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"UqhzirmamNWj"},"source":["# Create a small net and some toy data to check your implementations.\n","# Note that we set the random seed for repeatable experiments.\n","\n","input_size = 4\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","\n","def init_toy_model():\n","    np.random.seed(0)\n","    return bmodel(input_size, hidden_size, num_classes)\n","\n","def init_toy_data():\n","    np.random.seed(1)\n","    X = 10 * np.random.randn(num_inputs, input_size)\n","    y = np.array([0, 1, 2, 2, 1])\n","    return X, y\n","\n","toy_model = init_toy_model()\n","train_X, train_Y = init_toy_data()\n","validation_X, validation_Y = init_toy_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clIyJmUFJg0H"},"source":["Cuda is Compute Unified Device Architecture, which can achieve parallel computing. It will improve our learning speed in the parameter update by using GPU rather than CPU."]},{"cell_type":"code","metadata":{"id":"qNhcttlxJSOk"},"source":["# Check whether there is a gpu for cuda\n","train_on_gpu = cuda.is_available()\n","print(f'Train on gpu: {train_on_gpu}')\n","\n","# Number of gpus\n","if train_on_gpu:\n","    gpu_count = cuda.device_count()\n","    print(f'{gpu_count} gpus detected.')\n","    if gpu_count > 1:\n","        multi_gpu = True\n","    else:\n","        multi_gpu = False\n","else:\n","    multi_gpu = False\n","print(train_on_gpu,multi_gpu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFXQ5rioKcOP"},"source":["###Data Preparation\n","Next, we need to organize the data before we load them into the network. In Pytorch, we use Tensor as the data structure for computing, and we use \"dataloaders\" to achieve mini-batch from the whole dataset."]},{"cell_type":"code","metadata":{"id":"kOcdfL03KVL6"},"source":["# Datasets organization\n","batch_size = 1\n","\n","# Transfer the data from numpy to tensor\n","data = {\n","    'train':\n","    TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_Y).float()),\n","    'valid':\n","    # please define your validation dataset\n","    ######################\n","    ### YOUR CODE HERE ###\n","    ######################\n","    \n","    #####################\n","    ### YOUR CODE END ###\n","    #####################\n","\n","}\n","\n","\n","# Dataloader iterators, make sure to shuffle\n","dataloaders = {\n","    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True,num_workers=10),\n","    'valid':\n","    # please define your validation data loader\n","    ######################\n","    ### YOUR CODE HERE ###\n","    ######################\n","    \n","    ######################\n","    ### YOUR CODE END ####\n","    ######################\n","}\n","\n","\n","\n","# Iterate through the dataloader once\n","trainiter = iter(dataloaders['train'])\n","features, labels = next(trainiter)\n","features.shape, labels.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JPQQ7iRaNgIp"},"source":["### Setting Learning Parameters\n","In our training process, we need to set up the loss function and optimizer tool."]},{"cell_type":"code","metadata":{"id":"Y6PbdfxENdOJ"},"source":["# Set up your criterion and optimizer\n","# You can use nn.CrossEntropyLoss() and optim.Adam()\n","#####################\n","### YOUR CODE HERE###\n","#####################\n","\n","####################\n","### YOUR CODE END###\n","####################\n","\n","for p in optimizer.param_groups[0]['params']:\n","    if p.requires_grad:\n","        print(p.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ck08UYlKcGtk"},"source":["## Task 2: Build the Training Pipeline"]},{"cell_type":"markdown","metadata":{"id":"f5ep_QqOOiNn"},"source":["### Training Process\n","We are now defining the train function. Please follow the instruction to finish each part."]},{"cell_type":"code","metadata":{"id":"vw16i2tlOcbt"},"source":["def train(model,\n","          criterion,\n","          optimizer,\n","          train_loader,\n","          valid_loader,\n","          save_file_name,\n","          max_epochs_stop=3,\n","          n_epochs=10,\n","          print_every=1):\n","    \"\"\"Train a PyTorch Model\n","\n","    Params\n","    --------\n","        model (PyTorch model): cnn to train\n","        criterion (PyTorch loss): objective to minimize\n","        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n","        train_loader (PyTorch dataloader): training dataloader to iterate through\n","        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n","        save_file_name (str ending in '.pt'): file path to save the model state dict\n","        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n","        n_epochs (int): maximum number of training epochs\n","        print_every (int): frequency of epochs to print training stats\n","\n","    Returns\n","    --------\n","        model (PyTorch model): trained cnn with best weights\n","        history (DataFrame): history of train and validation loss and accuracy\n","    \"\"\"\n","\n","    # Early stopping intialization\n","    epochs_no_improve = 0\n","    valid_loss_min = np.Inf\n","\n","    valid_max_acc = 0\n","    history = []\n","\n","    # Number of epochs already trained (if using loaded in model weights)\n","    try:\n","        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n","    except:\n","        model.epochs = 0\n","        print(f'Starting Training from Scratch.\\n')\n","\n","    overall_start = timer()\n","\n","    # Main loop\n","    for epoch in range(n_epochs):\n","\n","        # keep track of training and validation loss each epoch\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        train_acc = 0\n","        valid_acc = 0\n","\n","        # Set to training\n","        model.train()\n","\n","        start = timer()\n","\n","        # Training loop\n","        for ii, (data, target) in enumerate(train_loader):\n","            \n","            # Tensors to gpu, both model parameters, data, and target need to be tensors.\n","            # You can use .cuda() function\n","            ######################\n","            ### YOUR CODE HERE ###\n","            ######################\n","            \n","            #####################\n","            ### YOUR CODE END ###\n","            #####################\n","\n","            # Clear gradients\n","            optimizer.zero_grad()\n","\n","            # Forward path\n","            ######################\n","            ### YOUR CODE HERE ###\n","            ######################\n","            \n","            #####################\n","            ### YOUR CODE END ###\n","            #####################\n","\n","            # Loss function \n","            ######################\n","            ### YOUR CODE HERE ###\n","            ######################\n","            \n","            #####################\n","            ### YOUR CODE END ###\n","            #####################\n","\n","            # Backward path (backpropagation)\n","            ######################\n","            ### YOUR CODE HERE ###\n","            ######################\n","            \n","            #####################\n","            ### YOUR CODE END ###\n","            #####################\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            # Track train loss by multiplying average loss by number of examples in batch\n","            train_loss += loss.item() * data.size(0)\n","\n","            # Calculate accuracy by finding max log probability\n","            _, pred = torch.max(output, dim=1)\n","            correct_tensor = pred.eq(target.data.view_as(pred))\n","\n","            # Need to convert correct tensor from int to float to average\n","            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","\n","            # Multiply average accuracy times the number of examples in batch\n","            train_acc += accuracy.item() * data.size(0)\n","\n","            # Track training progress\n","            print(\n","                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n","                end='\\r')\n","\n","        # After training loops ends, start validation\n","        else:\n","            model.epochs += 1\n","\n","            # Don't need to keep track of gradients\n","            with torch.no_grad():\n","\n","                # Set to evaluation mode\n","                model.eval()\n","\n","                # Validation loop\n","                for data, target in valid_loader:\n","                    # Tensors to gpu\n","                    ######################\n","                    ### YOUR CODE HERE ###\n","                    ######################\n","                    \n","                    #####################\n","                    ### YOUR CODE END ###\n","                    #####################\n","\n","                    # Forward path\n","                    ######################\n","                    ### YOUR CODE HERE ###\n","                    ######################\n","                    \n","                    #####################\n","                    ### YOUR CODE END ###\n","                    #####################\n","\n","                    # Validation loss computation\n","                    ######################\n","                    ### YOUR CODE HERE ###\n","                    ######################\n","                    \n","                    #####################\n","                    ### YOUR CODE END ###\n","                    #####################\n","\n","                    # Multiply average loss times the number of examples in batch\n","                    valid_loss += loss.item() * data.size(0)\n","\n","                    # Calculate validation accuracy\n","                    _, pred = torch.max(output, dim=1)\n","                    correct_tensor = pred.eq(target.data.view_as(pred))\n","                    accuracy = torch.mean(\n","                    correct_tensor.type(torch.FloatTensor))\n","\n","                    # Multiply average accuracy times the number of examples\n","                    valid_acc += accuracy.item() * data.size(0)\n","\n","\n","                # Calculate average losses and Calculate average accuracy\n","                train_loss = train_loss / len(train_loader.dataset)\n","                valid_loss = valid_loss / len(valid_loader.dataset)\n","\n","                train_acc = train_acc / len(train_loader.dataset)\n","                valid_acc = valid_acc / len(valid_loader.dataset)\n","\n","                history.append([train_loss, valid_loss, train_acc, valid_acc])\n","\n","                # Print training and validation results\n","                if (epoch + 1) % print_every == 0:\n","                    print(\n","                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n","                    )\n","                    print(\n","                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n","                    )\n","\n","                # Save the model if validation loss decreases\n","                if valid_loss < valid_loss_min:\n","                    # Save model \n","                    # You can use torch.save()\n","                    ######################\n","                    ### YOUR CODE HERE ###\n","                    ######################\n","\n","                    #####################\n","                    ### YOUR CODE END ###\n","                    #####################\n","\n","                    # Track improvement\n","                    epochs_no_improve = 0\n","                    valid_loss_min = valid_loss\n","                    valid_best_acc = valid_acc\n","                    best_epoch = epoch\n","\n","                # Otherwise increment count of epochs with no improvement\n","                else:\n","                    epochs_no_improve += 1\n","                    # Trigger early stopping\n","                    if epochs_no_improve >= max_epochs_stop:\n","                        print(\n","                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n","                        )\n","                        total_time = timer() - overall_start\n","                        print(\n","                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n","                        )\n","\n","                        # Load the best state dict\n","                        # You can use model.load_state_dict()\n","                        ######################\n","                        ### YOUR CODE HERE ###\n","                        ######################\n","\n","                        #####################\n","                        ### YOUR CODE END ###\n","                        #####################\n","\n","                        # Attach the optimizer\n","                        model.optimizer = optimizer\n","\n","                        # Format history\n","                        history = pd.DataFrame(\n","                            history,\n","                            columns=[\n","                                'train_loss', 'valid_loss', 'train_acc',\n","                                'valid_acc'\n","                            ])\n","                        return model, history\n","\n","    # Attach the optimizer\n","    model.optimizer = optimizer\n","    # Record overall time and print out stats\n","    total_time = timer() - overall_start\n","    print(\n","        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_best_acc:.2f}%'\n","    )\n","    print(\n","        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n","    )\n","    # Format history\n","    history = pd.DataFrame(\n","        history,\n","        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n","    return model, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4ZuMIexG7oo"},"source":["## Task 3: Train the network"]},{"cell_type":"markdown","metadata":{"id":"p_dFSRgzmNWr"},"source":["### Train a Network With Toy Data\n"]},{"cell_type":"markdown","metadata":{"id":"AdRJcmzXReJT"},"source":["Well done! Once we finish our train process design, we can start to train our network with our toy dataset."]},{"cell_type":"code","metadata":{"id":"LFI9SHsuReqs"},"source":["from timeit import default_timer as timer\n","save_file_name = f'toy_model_best_model.pt'\n","train_on_gpu = cuda.is_available()\n","\n","model, history = train(toy_model,\n","    criterion,\n","    optimizer,\n","    dataloaders['train'], \n","    dataloaders['valid'],\n","    save_file_name=save_file_name,\n","    max_epochs_stop=3,\n","    n_epochs=500,\n","    print_every=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5oRuk5hSEYx"},"source":["Now, we can check the losses and accuracy during the training."]},{"cell_type":"code","metadata":{"id":"-yJuZ4W2SD1W"},"source":["plt.figure(figsize=(8, 6))\n","for c in ['train_loss', 'valid_loss']:\n","    plt.plot(\n","        history[c], label=c)\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Negative Log Likelihood')\n","plt.title('Training and Validation Losses')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nT_VflxLSPca"},"source":["plt.figure(figsize=(8, 6))\n","for c in ['train_acc', 'valid_acc']:\n","    plt.plot(\n","        100 * history[c], label=c)\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQyFOOa_SUZB"},"source":["### Train a Network With Cifar-10 Data\n","Now we may use the cifar-10 dataset to train our model. First, we will load the cifar-10 dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fID7AMV0TZUO","executionInfo":{"status":"ok","timestamp":1614233321022,"user_tz":360,"elapsed":2830,"user":{"displayName":"Ruining Deng","photoUrl":"","userId":"01845380453294540034"}},"outputId":"9b91e61d-2ea9-4acd-a0ba-cf5d405cb775"},"source":["from data_utils import load_CIFAR10\n","\n","def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the two-layer neural net classifier.   \n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = './datasets/'\n","    \n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","        \n","    # Subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","\n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis=0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","\n","    # Reshape data to rows\n","    X_train = X_train.reshape(num_training, -1)\n","    X_val = X_val.reshape(num_validation, -1)\n","    X_test = X_test.reshape(num_test, -1)\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","\n","# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","try:\n","   del X_train, y_train\n","   del X_test, y_test\n","   print('Clear previously loaded data.')\n","except:\n","   pass\n","\n","# Invoke the above function to get our data.\n","train_X, train_Y, validation_X, validation_Y, test_X, test_Y = get_CIFAR10_data()\n","print('Train data shape: ', train_X.shape)\n","print('Train labels shape: ', train_Y.shape)\n","print('Validation data shape: ', validation_X.shape)\n","print('Validation labels shape: ', validation_Y.shape)\n","print('Test data shape: ', test_X.shape)\n","print('Test labels shape: ', test_Y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train data shape:  (49000, 3072)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3072)\n","Validation labels shape:  (1000,)\n","Test data shape:  (1000, 3072)\n","Test labels shape:  (1000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9mJ6CTNDTbXq"},"source":["Since Cifar-10 has a larger size of data, which is harder to train, we need to increase our network parameters to solve this task. \n"]},{"cell_type":"code","metadata":{"id":"qTmBf_03TD2J"},"source":["input_size = 32 * 32 * 3\n","hidden_size = 50\n","num_classes = 10\n","\n","model = bmodel(input_size, hidden_size, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUvAVk2dULUx"},"source":["Use the same chunk above to organize our new data. Now we may need a bigger batch_size for training."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uz8yZR95UHq9","executionInfo":{"status":"ok","timestamp":1614233330449,"user_tz":360,"elapsed":851,"user":{"displayName":"Ruining Deng","photoUrl":"","userId":"01845380453294540034"}},"outputId":"df10a36d-47e6-41ce-96e9-8c65f8fcd97b"},"source":["# Datasets from folders\n","batch_size = 16\n","# Transfer the data from numpy to tensor\n","data = {\n","    'train':\n","    TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_Y).float()),\n","    'valid':\n","    ######################\n","    ### YOUR CODE HERE ###\n","    ######################\n","    \n","    #####################\n","    ### YOUR CODE END ###\n","    #####################\n","\n","}\n","\n","\n","# Dataloader iterators, make sure to shuffle\n","dataloaders = {\n","    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True,num_workers=10),\n","    'valid':\n","    ######################\n","    ### YOUR CODE HERE ###\n","    ######################\n","    \n","    ######################\n","    ### YOUR CODE END ###\n","    ######################\n","}\n","\n","\n","# Iterate through the dataloader once\n","trainiter = iter(dataloaders['train'])\n","features, labels = next(trainiter)\n","features.shape, labels.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([16, 3072]), torch.Size([16]))"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"LVBJq6SCYAV5"},"source":["Set up our criterion and optimizer for the new model."]},{"cell_type":"code","metadata":{"id":"pTpFzjGxX8cP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614233332680,"user_tz":360,"elapsed":226,"user":{"displayName":"Ruining Deng","photoUrl":"","userId":"01845380453294540034"}},"outputId":"2c607733-3ff5-4bd7-e22b-fc17963e3c6d"},"source":["# Set up your criterion and optimizer\n","######################\n","### YOUR CODE HERE ###\n","######################\n","\n","#####################\n","### YOUR CODE END ###\n","#####################"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([50, 3072])\n","torch.Size([50])\n","torch.Size([10, 50])\n","torch.Size([10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QfOM_4BkUiio"},"source":["Train your new network and get the results. At this time, the training process may need more time."]},{"cell_type":"code","metadata":{"id":"feL2jumrUhRJ"},"source":["from timeit import default_timer as timer\n","save_file_name = f'cifar_model_best_model.pt'\n","train_on_gpu = cuda.is_available()\n","\n","model, history = train(model,\n","    criterion,\n","    optimizer,\n","    dataloaders['train'], \n","    dataloaders['val'],\n","    save_file_name=save_file_name,\n","    max_epochs_stop=3,\n","    n_epochs=500,\n","    print_every=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jeZ2CzV6YN-2"},"source":["It seems that the process will end very early since there is no progress during the training. Let's print the results."]},{"cell_type":"code","metadata":{"id":"xlzAjeIDUohp"},"source":["plt.figure(figsize=(8, 6))\n","for c in ['train_loss', 'valid_loss']:\n","    plt.plot(\n","        history[c], label=c)\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Negative Log Likelihood')\n","plt.title('Training and Validation Losses')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwjRiBhWUsdy"},"source":["plt.figure(figsize=(8, 6))\n","for c in ['train_acc', 'valid_acc']:\n","    plt.plot(\n","        100 * history[c], label=c)\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhsY7oawmNWu"},"source":["##Task 4: Improve The Performance\n","As we have known in HW2, a two-layer-network can not get a good performance. Here, we may (1) add more layers to make the network deeper, or (2) replace your bmodel() with networks provided by PyTorch. https://pytorch.org/vision/0.8/models.html\n","You just need to do one of these two options.\n","\n","You can reuse the code you have from Task 2 and 3"]},{"cell_type":"code","metadata":{"id":"ZFcxylevmxrF"},"source":["######################\n","### YOUR CODE HERE ###\n","######################\n","\n","#####################\n","### YOUR CODE END ###\n","#####################"],"execution_count":null,"outputs":[]}]}